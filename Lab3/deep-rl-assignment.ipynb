{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment should be done in groups of 2 or 3 (preferably 3).\n",
    "\n",
    "This assignment consists of two parts. In the first part, your task is to solve a grid world environment using the REINFORCE algorithm. In the second part, your task is to balance a pendulum attached to a cart such that it stays upright. You will implement the A2C algorithm that will learn a control policy for the cart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports \n",
    "Imports required packages. Add additional packages if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.table import Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: GridWorld with REINFORCE (9 pts)\n",
    "Firstly, you will implement REINFORCE in the GridWorld environment. The agent starts from the center, and the four actions left, right, up and down move the agent in the appropriate directions deterministically. The corner states are terminal and have the corresponding rewards.\n",
    "\n",
    "**In this part, you should only use `numpy` operations (i.e., no automated differentiation with PyTorch).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The GridWorld Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already provide a basic implementation of the grid world environment which you will use,\n",
    "as well as the constants `ACTION_LEFT, ACTION_RIGHT, ACTION_UP, ACTION_DOWN`.\n",
    "You do not need to modify the class `GridWorld`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT MOIFY\n",
    "ACTION_LEFT = 0\n",
    "ACTION_RIGHT = 1\n",
    "ACTION_UP = 2\n",
    "ACTION_DOWN = 3\n",
    "\n",
    "def clip(x, a, b):\n",
    "    if x < a:\n",
    "        return a\n",
    "    if x > b:\n",
    "        return b\n",
    "    \n",
    "    return x\n",
    "\n",
    "class GridWorld:\n",
    "    \n",
    "    def __init__(self, size=3):\n",
    "        self.size = size\n",
    "        self.state = [0,0]\n",
    "        self.actions = [ACTION_LEFT, ACTION_RIGHT, ACTION_UP, ACTION_DOWN]\n",
    "        \n",
    "    def reset_world(self):\n",
    "        self.state = [0,0]\n",
    "    \n",
    "    def is_terminal(self):\n",
    "        return abs(self.state[0]) == self.size and abs(self.state[1]) == self.size\n",
    "    \n",
    "    def get_current_reward(self):\n",
    "        if self.state == [self.size, self.size] or \\\n",
    "           self.state == [-self.size, -self.size]:\n",
    "            return -1\n",
    "        elif self.state == [-self.size, self.size] or \\\n",
    "             self.state == [self.size, -self.size]:\n",
    "            return +1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def list_actions(self):\n",
    "        return self.actions\n",
    "    \n",
    "    def step(self, action):\n",
    "        if action not in self.actions:\n",
    "            raise NotImplementedError(\"Action not defined\")\n",
    "            \n",
    "        if self.is_terminal():\n",
    "            raise NotImplementedError(\"Epsiode terminated! Call reset_world first.\")\n",
    "\n",
    "        if action == ACTION_LEFT:\n",
    "            self.state[0] = clip(self.state[0]-1, -self.size, self.size)\n",
    "            \n",
    "        elif action == ACTION_RIGHT:\n",
    "            self.state[0] = clip(self.state[0]+1, -self.size, self.size)\n",
    "            \n",
    "        elif action == ACTION_UP:\n",
    "            self.state[1] = clip(self.state[1]+1, -self.size, self.size)\n",
    "            \n",
    "        elif action == ACTION_DOWN:\n",
    "            self.state[1] = clip(self.state[1]-1, -self.size, self.size)\n",
    "        \n",
    "    def get_coordinates(self):\n",
    "        s = np.array(self.state)\n",
    "        s[0] = s[0] + self.size\n",
    "        s[1] = self.size - s[1]\n",
    "        \n",
    "        return s\n",
    "    \n",
    "    def get_state(self):\n",
    "        return self.get_coordinates()[0] + \\\n",
    "               self.get_coordinates()[1] * (self.size * 2 + 1)\n",
    "    \n",
    "    def render_state(self):\n",
    "        N = self.size\n",
    "        x = self.state[0]\n",
    "        y = self.state[1]\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(5,5))\n",
    "        ax.set_axis_off()\n",
    "\n",
    "        tb = Table(ax, bbox=[0,0, 1, 1])\n",
    "\n",
    "        row_labels = np.arange(2 * N + 1)\n",
    "        col_labels = np.arange(2 * N + 1)\n",
    "        width, height = .5, .5\n",
    "\n",
    "        values = np.zeros((N*2+1, N*2+1))\n",
    "        values[0,0] = values[-1, -1] = 1\n",
    "        values[-1,0] = values[0, -1] = -1\n",
    "\n",
    "        cell_colors = ('white', 'whitesmoke')\n",
    "        font_colors = ('black', 'white')\n",
    "\n",
    "        for (row_idx, col_idx), cell_val in np.ndenumerate(values):\n",
    "            idx = (col_idx + row_idx) % 2\n",
    "            if cell_val !=  0.:\n",
    "                text = \"%.1f\" % cell_val\n",
    "                if cell_val > 0:\n",
    "                    c = \"lightblue\"\n",
    "                else:\n",
    "                    c = \"lightcoral\"\n",
    "            elif row_idx == N and col_idx == N:\n",
    "                text = \"start\"\n",
    "                c = \"antiquewhite\"\n",
    "            else:\n",
    "                text = \"\"\n",
    "                c = cell_colors[idx]\n",
    "\n",
    "            tb.add_cell(row_idx, col_idx, width, height,\n",
    "                        text=text,\n",
    "                        loc='center',\n",
    "                        facecolor=c)\n",
    "\n",
    "        for (row_idx, col_idx), cell_val in np.ndenumerate(values):\n",
    "            idx = (col_idx + row_idx) % 2\n",
    "            tb._cells[(row_idx, col_idx)]._text.set_color(font_colors[idx])\n",
    "\n",
    "        pos = plt.Circle((0.5 + 1/(2 * N + 1) * x, 0.5 + 1/(2 * N + 1) * y + .04), 0.01, color='black')\n",
    "        ax.add_patch(pos)\n",
    "        ax.add_table(tb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can plot the environment with `render_state()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GridWorld().render_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The state after the agent takes the action `ACTION_RIGHT`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world = GridWorld()\n",
    "world.reset_world()\n",
    "world.step(ACTION_RIGHT)\n",
    "world.render_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cells/states are numbered from 0 to 48, left to right, top to bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world.get_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, cells are given coordinates left to right, top to bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world.get_coordinates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct Parameterization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we will define the policy according to an exponential softmax distribution where action preferences are linear using feature vectors \\phi_{s,a}:\n",
    "$$\n",
    "\\pi_\\theta(a|s) = \\frac{\\exp{\\theta^\\top \\phi_{s,a}}}{\\sum_{a'} {\\exp{\\theta^\\top \\phi_{s,a'}}}}\n",
    "$$\n",
    "\n",
    "We will assume that the vectors $\\phi_{s,a} \\in \\mathbb{R}^{|\\mathcal{S}||\\mathcal{A}|}$ are encoded as standard unit basis vectors (i.e., one-hot encoding).\n",
    "First, we will create an appropriate encoding function for the states in GridWorld.\n",
    "\n",
    "For consistency, we assume the actions are mapped to integers with `ACTION_LEFT=0, ACTION_RIGHT=1, ACTION_UP=2, ACTION_DOWN=3`, and the cell states are given values from 0 to 48, starting from left to right, from the top row to the bottom row. \n",
    "Note that `GridWorld.get_state()` implements the correct order on grid cells.\n",
    "$\\phi_{s,a}$ is assumed to be the unit vector with only the entry at position $4\\times s + a$ equal to 1.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.1 (0.5 pt):** Create a function `get_features_onehot` that encodes each $\\phi_{s,a}$ with a one hot encoded vector in $\\mathbb{R}^{|\\mathcal{S}||\\mathcal{A}|} = \\mathbb{R}^{196}$ as a numpy vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_onehot(state:int, action:int):\n",
    "    \"\"\"Given the tuple (x,y) indicating the coordinates on the grid world, return one hot encoded features\n",
    "    \n",
    "    Assume actions are numbered from 0 to 3, and states are numbered from 0 to 48\n",
    "    as explained above.\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: FILL IN YOUR CODE HERE!\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.2 (0.5 pt):** Implement an initialisation for the parameters $\\theta \\in \\mathbb{R}^{196}$ as a numpy vector that you think is appropriate for efficient exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_parameters():\n",
    "    \"\"\"Returns vector of size 196 of initial parameters theta\"\"\"\n",
    "    \n",
    "    # TODO: FILL IN YOUR CODE HERE!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.3 (2 pts):** \n",
    "Calculate the partial derivatives $\\frac{\\partial \\pi_\\theta(a|s)}{\\partial \\theta}$.\n",
    "First, write down the analytic formula for the derivative *in the Markdown cell below (use LaTeX-math mode for equations, etc.).* Show your computation steps! \n",
    "\n",
    "\n",
    "Then, using your expression fill in `direct_parameterisation_derivative`, that will return the vector $\\frac{\\partial \\pi_\\theta(a|s)}{\\partial \\theta}$ given a value of $\\theta$ and action-state pair $a,s$.\n",
    "\n",
    "**Hint:** For the analytic part, you may write down the formula for $\\frac{\\partial \\pi_\\theta(a|s)}{\\partial \\theta_{a',s'}}$, i.e., the partial derivative with respect to the entry of $\\theta$ corresponding to any action-state pair $a', s'$.\n",
    "\n",
    "**Hint:** The function `direct_parameterisation_derivative` should return a vector of size $4 |S| = 4 \\times 7^2=196$, since you have 4 actions and $7^2$ states.\n",
    "\n",
    "**Hint:** Only 4 entries of $\\frac{\\partial \\pi_\\theta(a|s)}{\\partial \\theta} \\in \\mathbb{R}^{196}$ should be nonzero for fixed $a,s$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def direct_parameterisation_gradient(action: int, state: int, theta: np.ndarray):\n",
    "    \"\"\"Returns the partial derivatives of the policy with respect to theta\n",
    "    \n",
    "    Assume actions are numbered from 0 to 3, and states are numbered from 0 to 48\n",
    "    as explained above.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: FILL IN YOUR CODE HERE!\n",
    "    \n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below helps you check if your derivative implementation is correct.\n",
    "In case your implementation works, the code should print success.\n",
    "You should not modify the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "# checks the code above\n",
    "torch.manual_seed(0)\n",
    "success = True\n",
    "N = 10\n",
    "err = 0.\n",
    "\n",
    "for i in range(N):\n",
    "    theta = torch.rand((4*49))\n",
    "    theta.requires_grad = True\n",
    "    \n",
    "    for state in range(49):\n",
    "        for action in range(4):\n",
    "            by_state = torch.exp(theta).reshape(49,4)\n",
    "            out_prob = by_state / by_state.sum(-1, keepdim=True)\n",
    "            out_prob.reshape(-1)[action + state*4].backward()\n",
    "\n",
    "            answer = direct_parameterisation_gradient(action, state, theta.detach().cpu().numpy())\n",
    "            \n",
    "            err += np.abs(answer - theta.grad.cpu().numpy()).sum()\n",
    "            \n",
    "            if not np.allclose(answer, theta.grad.cpu().numpy()):\n",
    "                success = False\n",
    "                \n",
    "            theta.grad.zero_()\n",
    "    \n",
    "if success:\n",
    "    print(f\"Success: errors within tolerance. Mean abs. error: {err / (4 * 49 * N)}\")\n",
    "else:\n",
    "    print(f\"Fail: errors not within tolerance. Mean abs. error: {err / (4 * 49 * N)}\")\n",
    "    \n",
    "del theta, by_state, out_prob, answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REINFORCE Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.4 (0.5 pt):** In the lectures, you have been introduced to the REINFORCE algorithm. Explain in a few sentences what the regular REINFORCE objective aims to optimise and how it is achieved only using sample and no prior information on the MDP dynamics. \n",
    "\n",
    "*Write in the Markdown cell below (use LaTeX-math mode for equations, etc.).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.5 (3 pts):** Using your functions implemented in Task 1.1, 1.2 and 1.3, fill in the `ReinforceLearner` class. You can use the already implemented methods of the GridWorld class: \n",
    "- `reset_world`: reset the world to starting state\n",
    "- `is_terminal`: is the current state terminal\n",
    "- `get_current_reward`: reward of the current state\n",
    "- `step`: given an action among `ACTION_LEFT, ACTION_RIGHT, ACTION_UP, ACTION_DOWN`, takes a step in the simulation\n",
    "- `get_state`: get current state, as an integer from 0 to 48\n",
    "- `get_coordinates`: get current coordinates in table as a tuple (x,y)\n",
    "- `render_state`: render the current state of the grid world, useful for debugging\n",
    "\n",
    "**Hint:** You do not need to implement an exploration strategy with the appropriate intialisation, so only vanilla policy gradient steps should be sufficient for convergence with an appropriate learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReinforceLearner:\n",
    "\n",
    "    def __init__(self, learning_rate, gamma):\n",
    "        \"\"\"Intialisation code\"\"\"\n",
    "        \n",
    "        self.world = GridWorld()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.theta = get_initial_parameters()\n",
    "        \n",
    "        ## TODO: you may use other variables or inputs as you hyper-parameters\n",
    "        \n",
    "    def get_action_prob(self, state:int):\n",
    "        \"\"\"Given the state, return the probability of taking each of the 4 actions\"\"\"\n",
    "        \n",
    "        ## TODO: FILL IN YOUR CODE HERE\n",
    "        \n",
    "        pass\n",
    "        \n",
    "    \n",
    "    def train(self, n_updates:int, n_trajectories:int):\n",
    "        \"\"\"Run the training procedure for n_updates steps\n",
    "        \n",
    "        n_trajectories: how many trajectories should be sampled each REINFORCE step\n",
    "        n_updates: how many parameters updates to make (each parameter update would sample n_trajectories)\"\"\"\n",
    "        \n",
    "        rewards = []\n",
    "        for episode in range(n_updates):\n",
    "            rewards.append(self.reinforce_step(n_trajectories))\n",
    "                \n",
    "        return rewards\n",
    "        \n",
    "    def reinforce_step(self, n_trajectories: int):\n",
    "        \"\"\"Run a single gradient step for REINFORCE (without baseline) using n_trajectories many trajectories for estimation.\n",
    "        \n",
    "        Also return the average reward observed\"\"\"\n",
    "        \n",
    "        ## TODO: FILL IN YOUR CODE HERE\n",
    "        \n",
    "        pass\n",
    "        \n",
    "        \n",
    "    def evaluate(self, render: bool = True):\n",
    "        \"\"\"Evaluate current algorithm for a single episode and return.\n",
    "        \n",
    "        You should \n",
    "        (a) Print the total reward from the episode.\n",
    "        (b) Visualise the whole state history. (Hint: You can use multiple calls to GridWorld.render_state())\"\"\"\n",
    "        \n",
    "        ## TODO: FILL IN YOUR CODE HERE\n",
    "        \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.6 (1 pt):** Run the above code until convergence, and evaluate your strategy at the end.\n",
    "Plot the average reward observed per epoch during training.\n",
    "You made need to tune your hyper-parameters. You policy should consistently visit the postive rewards to solve the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run your implementation for 1.5 here. \n",
    "# Remember to evaluate it at the end. Show your evalutation (including visualization of state history)!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Plot the average reward observed per epoch during training here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.7 (1 pt):** The code above for REINFORCE does not implement an explicit exploration strategy.\n",
    "Explain how the exploration-exploitation trade-off was circumvented by your choice of parameters intialisation in a few sentences, and how this approach could fail for some initialisations.\n",
    "\n",
    "*Write in the Markdown cell below (use LaTeX-math mode for equations, etc.).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.8 (0.5 pt):** In a few sentences explain the shortcomings of the direct parametrisation.\n",
    "\n",
    "*Write in the Markdown cell below (use LaTeX-math mode for equations, etc.).*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Part 2: CartPole with A2C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Cart Pole Environment\n",
    "\n",
    "The [CartPole](https://gymnasium.farama.org/environments/classic_control/cart_pole/) environment is a classical problem where a pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum is placed upright on the cart and the goal is to balance the pole by applying forces in the left and right direction on the cart.\n",
    "\n",
    "Since the focus of this task is the implementation of reinforcement\n",
    "learning algorithms, it is not necessary to have a detailed understanding of the mechanics of the\n",
    "Cart Pole environment beyond the observation and action space sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the environment\n",
    "Initialize the Cart Pole environment in the Gymnasium library to start learning a policy for it. You can read more about the Gymnasium library [here](https://gymnasium.farama.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Intro\n",
    "\n",
    "We also introduce basic functionality for PyTorch.\n",
    "PyTorch is a widely used automatic differentiation library, which is very useful for training neural networks.\n",
    "This part is also for guidance and does not include questions.\n",
    "While we do introduce the basics of PyTorch, you are encouraged to explore more about the library following the official [tutorials](https://pytorch.org/tutorials/beginner/basics/intro.html) in case you haven't used it before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch is built on top of the class `torch.Tensor`, which implements many operations on vectors, matrices and higher dimensional tensors.\n",
    "The functionality of operations on torch tensors closely mirrors the operations from the `numpy` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.zeros(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_matrix = [[1., 2.],[3., 4.]]\n",
    "torch.tensor(a_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.matmul(torch.tensor(a_matrix), \n",
    "             torch.tensor([4., 5.]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What makes torch special is that it implements automated differentiation by tracking the operation history on tensors.\n",
    "Namely, if a `torch.Tensor` has the propery `.requires_grad` set to `True`, torch will automatically track operations made on that tensor.\n",
    "A final `.backward()` call to a computation result will compute partial derivatives of the variable with respect to all tensors in the tracked computational graph.\n",
    "This is particularly useful for tracking derivatives with respect to a loss function, to enable simple gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example from https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html\n",
    "\n",
    "x = torch.ones(5)  # input tensor\n",
    "y = torch.zeros(3)  # expected output\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x, w)+b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# differentiate with respect to loss\n",
    "loss.backward()\n",
    "# the computed partial derivatives of w\n",
    "w.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prevent tracking history, you can also wrap a piece of code in `with torch.no_grad():`. \n",
    "This can be particularly helpful when evaluating a model on new data, when training (and hence differentiation) is not required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more advanced modelling (as will be required by this exercise), it is usually useful to implement classes as children of `nn.Module`.\n",
    "These implement the function `forward`, which returns the output of the forward pass of a model given inputs.\n",
    "Modules have `nn.Parameter`, which will be the trainable parameters of the models.\n",
    "Most useful sub-modules are already provided by the torch package: see for instance `nn.Linear`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, torch implements many standard optimizers: for instance `torch.optim.SGD`, `torch.optim.Adam`.\n",
    "These are typically used to automatically take care of training once partial derivatives are computed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantage Actor-Critic (A2C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more stable alternative to vanilla policy gradients via REINFORCE is the so-called A2C algorithm, which is a actor-critic method.\n",
    "First, you should familiarise yourself with the A2C algorithm reading the original [paper](https://arxiv.org/pdf/1602.01783.pdf).\n",
    "\n",
    "**Task 2.1 (3 pts):** Explain in a few sentences each, \n",
    "\n",
    "1. how the A2C algorithm addresses the shortcomings of vanilla policy gradient methods,\n",
    "2. the difference between A2C and A3C,\n",
    "3. why the A3C algorithm could be expected to perform even better than A2C.\n",
    "\n",
    "*Write in the Markdown cell below (use LaTeX-math mode for equations, etc.).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Parameterization\n",
    "\n",
    "We will use neural networks to parameterize both the value function and the policy.\n",
    "The `ActorNet` defines the policy $\\pi_\\theta(a|s)$. Given state $s$ as input, it will output a distribution over the action space. \n",
    "The `CriticNet` defines the approximation to the value function, which could be denoted $\\hat{V}_w(s)$. Given state $s$ as input, it will output an approximation of the state-value. \n",
    "\n",
    "You can play with different network architectures and the [activation](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity) functions.\n",
    "Note that the `ActorNet` should produce a probability distribution over the action space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.2 (0.5 pt):** Design your own neural networks for `ActorNet` and `CriticNet` in the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNet(torch.nn.Module):\n",
    "    def __init__(self, num_state: int, num_action: int):\n",
    "        super(ActorNet, self).__init__()\n",
    "        \n",
    "        # TODO: FILL IN YOUR CODE HERE\n",
    "        # see torch.nn.Sequential, for a hint\n",
    "        \n",
    "       \n",
    "    # return a probability distribution over the action space\n",
    "    def forward(self, state):\n",
    "        # TODO: FILL IN YOUR CODE HERE\n",
    "        pass\n",
    "\n",
    "\n",
    "class CriticNet(torch.nn.Module):\n",
    "    def __init__(self, num_state: int):\n",
    "        super(CriticNet, self).__init__()\n",
    "        \n",
    "        # TODO: FILL IN YOUR CODE HERE\n",
    "        # see torch.nn.Sequential, for a hint\n",
    "        \n",
    "       \n",
    "    # return a single value\n",
    "    def forward(self, state):\n",
    "        # TODO: FILL IN YOUR CODE HERE\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Actor and Critic\n",
    "We train both the actor and critic nets by interacting with the environment.\n",
    "You implement the A2C training process below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.3 (4 pts):** \n",
    "Fill in and run the code below to implement A2C in the Cart Pole environment.\n",
    "You should play with your hyper-parameters as well, to enable efficient learning.\n",
    "You may play with different learning rates and optimizers.\n",
    "For different optimizers in PyTorch, see [here](https://pytorch.org/docs/stable/optim.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class A2C():\n",
    "    \n",
    "    def __init__(self,env):\n",
    "        \"\"\"Intialisation code\"\"\"\n",
    "        # initiate both nets\n",
    "        self.actor = ActorNet(env.observation_space.shape[0], \n",
    "                        env.action_space.n)\n",
    "        self.critic = CriticNet(env.observation_space.shape[0])\n",
    "\n",
    "        # set gamma and learning rates\n",
    "        self.gamma = ... # TODO\n",
    "        actor_lr = ... # TODO\n",
    "        critic_lr = ... # TODO\n",
    "        \n",
    "        self.env = env\n",
    "\n",
    "        # TODO: also play with different optimizers\n",
    "        self.actor_opt = torch.optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic_opt = torch.optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "\n",
    "\n",
    "    def train(self, max_iter: int = 1000, max_episode_len: int = 500):\n",
    "\n",
    "\n",
    "        total_reward = []\n",
    "\n",
    "\n",
    "        pbar = tqdm.tqdm(range(max_iter), desc = \"Episode\")\n",
    "        for num_iter in pbar:\n",
    "            # at each iteration, we roll out an episode using current policy until it reach termination state or the maximum length\n",
    "            reward = []\n",
    "            \n",
    "            s,_ = self.env.reset()\n",
    "            for t in range(max_episode_len):\n",
    "                # given current state, the actor outputs two values as probability for each action\n",
    "                state = torch.tensor(s, dtype=torch.float).view(1, -1)\n",
    "                prob = self.actor(state)\n",
    "                # create the distribution as current policy\n",
    "                pi = ... # TODO\n",
    "                # sample one action from this policy\n",
    "                a = ... # TODO\n",
    "                \n",
    "                # interact with the environment\n",
    "                s, r, is_terminal, _, _ = self.env.step(a.item())      \n",
    "                reward.append(r)\n",
    "                \n",
    "                # TODO: what information will be used?\n",
    "                # hints: outputs of the actor and critic nets\n",
    "                \n",
    "                \n",
    "                if is_terminal:\n",
    "                    break\n",
    "                    \n",
    "            # store total_reward, do not modify\n",
    "            total_reward.append(sum(reward))\n",
    "            \n",
    "            pbar.set_postfix_str(f\"Last reward: {total_reward[-1]}, Mean last {min(len(total_reward),100)} episodes: {sum(total_reward[-100:])/min(len(total_reward),100):.2f}\")\n",
    "            # TODO: use the episode to update parameters\n",
    "            # for the actor, you can use policy gradient method (with baseline). What is the loss?\n",
    "            # for the critic, you can use Monte-Carlo, TD(0), TD(\\lambda). What is the loss?\n",
    "            \n",
    "            # TODO: add your implementation here\n",
    "            \n",
    "\n",
    "            self.actor_opt.zero_grad()\n",
    "            actor_loss = ... # TODO\n",
    "            actor_loss.backward()\n",
    "            self.actor_opt.step()\n",
    "            \n",
    "\n",
    "            self.critic_opt.zero_grad()    \n",
    "            critic_loss = ... # TODO\n",
    "            critic_loss.backward()\n",
    "            self.critic_opt.step()\n",
    "\n",
    "        self.env.close()\n",
    "        \n",
    "        return total_reward\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run your implementation here. \n",
    "max_iter = 1000\n",
    "max_episode_len = 500\n",
    "\n",
    "a2c = A2C(env)\n",
    "\n",
    "total_reward = a2c.train(max_iter, max_episode_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Useful hints:**\n",
    "\n",
    "1. See [here](https://pytorch.org/docs/stable/distributions.html) for distributions in PyTorch. You may find the function `log_prob()` useful. You can easily compute $\\log\\pi(a|s)$ and its gradient using the function.\n",
    "\n",
    "2. To optimize a model in PyTorch, you first define the loss function, then call the corresponding optimizer and perform backpropagation. For example, to optimize over the value function approximation, first let $\\text{loss}=\\|V_w-V^\\pi\\|^2$ and then call `loss.backward()` to compute the gradient.\n",
    "\n",
    "3. You may find the function `detach()` defined in PyTorch useful if you need to call `backward()` multiple times on the same variables without storing the computational graph every time.\n",
    "\n",
    "4. Maximizing $f(x)$ is equivalent to minimizing $-f(x)$. The default optimizers in PyTorch perform minimization.\n",
    "\n",
    "5. You may find the classes `torch.nn.Softmax`, `torch.nn.ReLU`, `torch.nn.Tanh` useful when implementing a neural network architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze your results\n",
    "You can run the codes below to test your algorithm.\n",
    "These will help you understand if your algorithm is working, and will help us grade your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will visualize your learnt policy on the Cart Pole environment\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "env_render = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "for num_episode in range(1):\n",
    "    s,_ = env_render.reset()\n",
    "    tot_reward = []\n",
    "    for t in range(max_episode_len):\n",
    "\n",
    "        state = torch.tensor(s, dtype=torch.float).view(1, -1)\n",
    "        \n",
    "        prob = a2c.actor(state)\n",
    "        policy = torch.distributions.categorical.Categorical(prob)\n",
    "        action = policy.sample()\n",
    "        s, r, is_terminal, _,_ = env_render.step(action.item())\n",
    "        tot_reward.append(r)\n",
    "        if is_terminal:\n",
    "            print(\"Episode {} terminated after {} steps with reward {}. \".format(num_episode, t+1, sum(tot_reward)))\n",
    "            break\n",
    "        clear_output(wait=True)\n",
    "        plt.imshow(env_render.render())\n",
    "        plt.show()\n",
    "        \n",
    "env_render.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a simple criteria to judge whether you solve the problem.\n",
    "We will compute an average of the total reward over the previous 100 episodes at each iteration.\n",
    "If there exists an iteration with average total reward larger than a threshold, as shown below, we will give you full grades.\n",
    "That is, the problem is solved when the average reward is greater than or equal to 475 over 100 consecutive trials.\n",
    "\n",
    "Note that `CartPole-v1` has a termination condition of 500 timesteps. It’s done so that one episode doesn’t take forever.\n",
    "So we say that if a policy can balance a pole for 500 time steps (and achieve 500 reward) it’s probably good enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code should print success\n",
    "success_flag = False\n",
    "buffer_len = 100\n",
    "buffer_sum = sum(total_reward[:buffer_len])\n",
    "avg_reward = [buffer_sum / buffer_len]\n",
    "\n",
    "for num_iter in range(buffer_len, max_iter):\n",
    "    buffer_sum += total_reward[num_iter]\n",
    "    buffer_sum -= total_reward[num_iter - buffer_len]\n",
    "    buffer_avg = buffer_sum / buffer_len\n",
    "    avg_reward.append(buffer_avg)\n",
    "    \n",
    "    if buffer_avg >= env.spec.reward_threshold and not success_flag:\n",
    "        print('Successfully solve the problem in {} iterations.'.format(num_iter + 1))\n",
    "        success_flag = True\n",
    "        \n",
    "if not success_flag:\n",
    "    print('Unfortunately the agent is not smart enough.')\n",
    "\n",
    "# plot the average reward curve through the training process\n",
    "plt.plot(range(buffer_len-1, len(total_reward)), avg_reward)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy regularized A2C\n",
    "\n",
    "The exploration-exploition trade-off is also a significant problem to be solved in the context of policy gradients.\n",
    "There have been numerous approaches in literature aimed towards maximizing exploration in a principled and theoretically meaningful manner.\n",
    "One such example is the entropy regularised policy gradient objective:\n",
    "$$\n",
    "J_\\tau(\\theta) = J(\\theta) + \\tau \\mathbb{E}_{s \\sim d^\\pi_s}\\left[-\\sum_a \\pi_\\theta(a|s) \\log \\pi_\\theta(a|s)\\right],\n",
    "$$\n",
    "where $J(\\theta)$ is the vanilla policy gradient objective and $\\tau$ is a hyperparameter controlling regularisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.4 (0.5 pt):** Explain why entropy regularization is relevant for encouraging exploration.\n",
    "\n",
    "*Write in the Markdown cell below (use LaTeX-math mode for equations, etc.).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.5 (1 pt):** Modify the training code from above to demonstrate A2C with entropy regularization. Implement a new loss function that returns your modified loss with entropy regularization. Use entropy regularisation on the CartPole task. Visualize your results and show that you have solved the problem. Compare to your results without entropy regularization in **Task 2.3**.\n",
    "\n",
    "Also feel free to explore different values of $\\tau$ and it's effect on the policy optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class A2CEntropy():\n",
    "    \n",
    "    def __init__(self,env):\n",
    "        \"\"\"Intialisation code\"\"\"\n",
    "        # initiate both nets\n",
    "        self.actor = ActorNet(env.observation_space.shape[0], \n",
    "                        env.action_space.n)\n",
    "        self.critic = CriticNet(env.observation_space.shape[0])\n",
    "\n",
    "        # set gamma and learning rates\n",
    "        self.gamma = ... # TODO\n",
    "        actor_lr = ... # TODO\n",
    "        critic_lr = ... # TODO\n",
    "        \n",
    "        self.env = env\n",
    "\n",
    "        # TODO: also play with different optimizers\n",
    "        self.actor_opt = torch.optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic_opt = torch.optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "\n",
    "\n",
    "    def train(self, max_iter: int = 1000, max_episode_len: int = 500):\n",
    "\n",
    "\n",
    "        total_reward = []\n",
    "\n",
    "\n",
    "        pbar = tqdm.tqdm(range(max_iter), desc = \"Episode\")\n",
    "        for num_iter in pbar:\n",
    "            # at each iteration, we roll out an episode using current policy until it reach termination state or the maximum length\n",
    "            reward = []\n",
    "            \n",
    "            s,_ = self.env.reset()\n",
    "            for t in range(max_episode_len):\n",
    "                # given current state, the actor outputs two values as probability for each action\n",
    "                state = torch.tensor(s, dtype=torch.float).view(1, -1)\n",
    "                prob = self.actor(state)\n",
    "                # create the distribution as current policy\n",
    "                pi = ... # TODO\n",
    "                # sample one action from this policy\n",
    "                a = ... # TODO\n",
    "                \n",
    "                # interact with the environment\n",
    "                s, r, is_terminal, _, _ = self.env.step(a.item())      \n",
    "                reward.append(r)\n",
    "                \n",
    "                # TODO: what information will be used?\n",
    "                # hints: outputs of the actor and critic nets\n",
    "                \n",
    "                \n",
    "                if is_terminal:\n",
    "                    break\n",
    "                    \n",
    "            # store total_reward, do not modify\n",
    "            total_reward.append(sum(reward))\n",
    "            \n",
    "            pbar.set_postfix_str(f\"Last reward: {total_reward[-1]}, Mean last {min(len(total_reward),100)} episodes: {sum(total_reward[-100:])/min(len(total_reward),100):.2f}\")\n",
    "            # TODO: use the episode to update parameters\n",
    "            # for the actor, you can use policy gradient method (with baseline). What is the loss?\n",
    "            # for the critic, you can use Monte-Carlo, TD(0), TD(\\lambda). What is the loss?\n",
    "            \n",
    "            # TODO: add your implementation here\n",
    "            \n",
    "\n",
    "            self.actor_opt.zero_grad()\n",
    "            actor_loss = ... # TODO\n",
    "            actor_loss.backward()\n",
    "            self.actor_opt.step()\n",
    "            \n",
    "\n",
    "            self.critic_opt.zero_grad()    \n",
    "            critic_loss = ... # TODO\n",
    "            critic_loss.backward()\n",
    "            self.critic_opt.step()\n",
    "\n",
    "        self.env.close()\n",
    "        \n",
    "        return total_reward\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run your implementation here. \n",
    "max_iter_entropy = 1000\n",
    "max_episode_len_entropy = 500\n",
    "\n",
    "a2c_entropy = A2CEntropy(env)\n",
    "\n",
    "total_reward_entropy = a2c_entropy.train(max_iter_entropy, max_episode_len_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code should print success\n",
    "success_flag = False\n",
    "buffer_len = 100\n",
    "buffer_sum = sum(total_reward_entropy[:buffer_len])\n",
    "avg_reward = [buffer_sum / buffer_len]\n",
    "\n",
    "for num_iter in range(buffer_len, max_iter):\n",
    "    buffer_sum += total_reward_entropy[num_iter]\n",
    "    buffer_sum -= total_reward_entropy[num_iter - buffer_len]\n",
    "    buffer_avg = buffer_sum / buffer_len\n",
    "    avg_reward.append(buffer_avg)\n",
    "    \n",
    "    if buffer_avg >= env.spec.reward_threshold and not success_flag:\n",
    "        print('Successfully solve the problem in {} iterations.'.format(num_iter + 1))\n",
    "        success_flag = True\n",
    "        \n",
    "if not success_flag:\n",
    "    print('Unfortunately the agent is not smart enough.')\n",
    "\n",
    "# plot the average reward curve through the training process\n",
    "plt.plot(range(buffer_len-1, len(total_reward_entropy)), avg_reward)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize your results and compare it to the results without entropy in Task 2.3\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter-env",
   "language": "python",
   "name": "jupyter-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
