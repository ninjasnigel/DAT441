{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bandit Assignment\n",
    "\n",
    "This assignment should be done in groups of 3 and consists of a number of implementation and theory problems based on the topics discussed in the lectures and the course literature (specifically, **version 5** on arXiv):\n",
    "\n",
    "[Bandits] *Aleksandrs Slivkins, [Introduction to Multi-Armed Bandits](https://arxiv.org/pdf/1904.07272v5.pdf), Found. Trends Mach. Learn. 12(1-2): 1-286 (2019)*\n",
    "\n",
    "In the implementation problems **(1, 2, 3 and 5)**, you will implement multi-armed bandit algorithms from the [Bandits] book and use them in a provided multi-armed bandit environment. These problems will be graded based on the correctness of the code.\n",
    "\n",
    "In the theory problems **(4 and 6)**, you will derive some properties of the algorithms. These problems will be graded based on the correctness of the arguments.\n",
    "\n",
    "You may use the python libraries imported below (*numpy*, *scipy.stats* and *pandas*).\n",
    "\n",
    "The assignment should be handed in as an updated notebook. The entire notebook should be run before it is handed in, so that the plots are visible. Ensure that it is completely runnable, in the case that we want to reproduce the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below contains imports. It may not be modified!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SEED = 150\n",
    "ITERATIONS = 10\n",
    "K = 100\n",
    "T = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below contains the bandit environment and may not be modified!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "class Environment:\n",
    "    def __init__(self, K=10, seed=0):\n",
    "        self.random_state = np.random.RandomState(seed=seed)\n",
    "        self.mu = st.beta.rvs(a=1, b=1, size=K, random_state=self.random_state)\n",
    "        \n",
    "    def expected_value(self, a):\n",
    "        return self.mu[a]\n",
    "        \n",
    "    def perform_action(self, a):\n",
    "        return st.bernoulli.rvs(self.mu[a], random_state=self.random_state)\n",
    "        \n",
    "    def optimal_action(self):\n",
    "        return np.argmax(self.mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below contains the bandit algorithm base class and may not be modified!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "class BanditAlgorithmBase:\n",
    "    def select_action(self):\n",
    "        pass\n",
    "    \n",
    "    def update(self, action, reward):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below contains the bandit experiment and may not be modified!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "class Experiment:\n",
    "    def __init__(self, environment, bandit_algorithm):\n",
    "        self.environment = environment\n",
    "        self.bandit_algorithm = bandit_algorithm\n",
    "        \n",
    "    def run_experiment(self, T=100):\n",
    "        instant_regrets = np.zeros(T)\n",
    "        for t in range(0, T):\n",
    "            action = self.bandit_algorithm.select_action()\n",
    "            reward = self.environment.perform_action(action)\n",
    "            self.bandit_algorithm.update(action, reward)\n",
    "            \n",
    "            optimal_action = self.environment.optimal_action()\n",
    "            instant_regret = self.environment.expected_value(optimal_action) - self.environment.expected_value(action)\n",
    "            instant_regrets[t] = instant_regret\n",
    "        cumulative_regrets = np.cumsum(instant_regrets)\n",
    "        return (instant_regrets, cumulative_regrets)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below contains a function for repeated experiments with a provided bandit algorithm, averaging regret over the runs. It may not be modified!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "def run_repeated_experiments(bandit_algorithm_class, seed):\n",
    "    instant_regrets = []\n",
    "    cumulative_regrets = []\n",
    "    for i in range(ITERATIONS):\n",
    "        bandit_algorithm = bandit_algorithm_class(T, K)\n",
    "        environment = Environment(K, seed+i+1)\n",
    "        experiment = Experiment(environment, bandit_algorithm)\n",
    "\n",
    "        instant_regrets_i, cumulative_regrets_i = experiment.run_experiment(T)\n",
    "        instant_regrets.append(instant_regrets_i)\n",
    "        cumulative_regrets.append(cumulative_regrets_i)\n",
    "    return pd.DataFrame(data={'t': np.arange(1, T+1),\n",
    "                             'instant_regret': np.mean(np.vstack(np.array(instant_regrets)), axis=0),\n",
    "                             'regret': np.mean(np.vstack(np.array(cumulative_regrets)), axis=0)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Bandits (Chapter 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1 \n",
    "(3 points)\n",
    "\n",
    "Implement the *Explore-First* algorithm (**Algorithm 1.1** in [Bandits]) within the provided bandit algorithm template below. Use $N = \\left(\\frac{T}{K}\\right)^{2/3} \\cdot \\left( \\log T \\right)^{1/3}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExploreFirst(BanditAlgorithmBase):\n",
    "    def __init__(self, T, K):\n",
    "        \"\"\"\n",
    "        Constructor of the bandit algorithm\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        T : int\n",
    "            Horizon\n",
    "        K : int\n",
    "            Number of actions\n",
    "        \"\"\"\n",
    "        \n",
    "        # FILL IN CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def select_action(self):\n",
    "        \"\"\"\n",
    "        Select an action which will be performed in the environment in the \n",
    "        current time step\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        An action index (integer) in [0, K-1]\n",
    "        \"\"\"\n",
    "        \n",
    "        # FILL IN CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def update(self, action, reward):\n",
    "        \"\"\"\n",
    "        Update the bandit algorithm with the reward received from the \n",
    "        environment for the action performed in the current time step\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        action : int\n",
    "            An action index (integer) in [0, K-1]\n",
    "        reward : int\n",
    "            Reward (integer) in {0, 1} (Bernoulli rewards)\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # FILL IN CODE HERE\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the algorithm in the provided environment using the code below (averaging regret over 5 runs). The exploration and exploitation phases should be clearly visible in the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "np.random.seed(SEED)\n",
    "ef_df = run_repeated_experiments(ExploreFirst, SEED)\n",
    "ef_df.plot(x='t', y='regret', title='Explore-First')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2\n",
    "(3 points) \n",
    "\n",
    "Implement the $ \\epsilon_t $-*Greedy* algorithm (**Algorithm 1.2** in [Bandits]) within the provided bandit algorithm template below. Use $\\epsilon_t = \\min \\left\\{1,\\ t^{-1/3} \\cdot (K \\log t)^{1/3}\\right\\} $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonTGreedy(BanditAlgorithmBase):\n",
    "    def __init__(self, T, K):\n",
    "        \"\"\"\n",
    "        Constructor of the bandit algorithm\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        T : int\n",
    "            Horizon\n",
    "        K : int\n",
    "            Number of actions\n",
    "        \"\"\"\n",
    "        \n",
    "        # FILL IN CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def select_action(self):\n",
    "        \"\"\"\n",
    "        Select an action which will be performed in the environment in the \n",
    "        current time step\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        An action index (integer) in [0, K-1]\n",
    "        \"\"\"\n",
    "        \n",
    "        # FILL IN CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def update(self, action, reward):\n",
    "        \"\"\"\n",
    "        Update the bandit algorithm with the reward received from the \n",
    "        environment for the action performed in the current time step\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        action : int\n",
    "            An action index (integer) in [0, K-1]\n",
    "        reward : int\n",
    "            Reward (integer) in {0, 1} (Bernoulli rewards)\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # FILL IN CODE HERE\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the algorithm in the provided environment using the code below (averaging regret over 5 runs). The plot should show sublinear regret with respect to $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "np.random.seed(SEED)\n",
    "eg_df = run_repeated_experiments(EpsilonTGreedy, SEED)\n",
    "eg_df.plot(x='t', y='regret', title='Epsilon_t-Greedy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3\n",
    "(3 points) \n",
    "\n",
    "Implement the UCB1 algorithm (**Algorithm 1.5** in [Bandits]) within the provided bandit algorithm template below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCB1(BanditAlgorithmBase):\n",
    "    def __init__(self, T, K):\n",
    "        \"\"\"\n",
    "        Constructor of the bandit algorithm\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        T : int\n",
    "            Horizon\n",
    "        K : int\n",
    "            Number of actions\n",
    "        \"\"\"\n",
    "        \n",
    "        # FILL IN CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def select_action(self):\n",
    "        \"\"\"\n",
    "        Select an action which will be performed in the environment in the \n",
    "        current time step\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        An action index (integer) in [0, K-1]\n",
    "        \"\"\"\n",
    "        \n",
    "        # FILL IN CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def update(self, action, reward):\n",
    "        \"\"\"\n",
    "        Update the bandit algorithm with the reward received from the \n",
    "        environment for the action performed in the current time step\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        action : int\n",
    "            An action index (integer) in [0, K-1]\n",
    "        reward : int\n",
    "            Reward (integer) in {0, 1} (Bernoulli rewards)\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # FILL IN CODE HERE\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the algorithm in the provided environment using the code below (averaging regret over 5 runs). The plot should show sublinear regret with respect to $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "np.random.seed(SEED)\n",
    "ucb1_df = run_repeated_experiments(UCB1, SEED)\n",
    "ucb1_df.plot(x='t', y='regret', title='UCB1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4\n",
    "(6 points) \n",
    "\n",
    "This theory problem is based on **Exercise 1.1** in [Bandits]. The proofs in **Chapter 1** consider environments where the rewards are in the interval $[0,1]$. Consider the case when we have additional knowledge about about the problem and that we know that the rewards for each action are in the interval $\\left[\\frac{1}{2}, \\frac{1}{2} + \\epsilon\\right]$ for some fixed $\\epsilon \\in \\left(0, \\frac{1}{2}\\right)$. \n",
    "\n",
    "Consider a version of $\\text{UCB1}$ modified to utilize this knowledge (you do not need to specify the algorithm completely, just define the new confidence radius $r_t(a)$). For this algorithm and problem setting, prove that:\n",
    "\n",
    "$\\mathbb{E}\\left[R(t)\\right] \\leq \\frac{2 \\epsilon t}{T^2} + 2 \\epsilon \\sqrt{2 K t \\log T}$\n",
    "\n",
    "**Instructions:** Use a version of Hoeffding Inequality with ranges (**Theorem A.2** in the [Bandits] book) to modify the confidence radius $r_t(a)$. Subsequently follow the steps of the analysis leading up to **Theorem 1.14** in [Bandits] to derive the regret bound, though show the actual constants instead of using big O notation:\n",
    "\n",
    "1. Define the clean event, like in **Section 1.3.1**, and lower bound the probability of the event.\n",
    "2. Start with the definition of the regret $\\mathbb{E}\\left[R(t)\\right]$, and perform a regret decomposition like on **Page 11** of **Section 1.3.2**.\n",
    "3. Bound the *gap* $\\Delta (a_t)$, like in **Section 1.3.3**.\n",
    "4. Complete the proof using the technique on **Page 12** of **Section 1.3.2**. Note that applying the bound in step 3 requires careful motivation.\n",
    "\n",
    "*Write the solution in the Markdown cell below (use LaTeX-math mode for equations, etc.).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WRITE SOLUTION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Bandits (Chapter 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5\n",
    "(3 points)\n",
    "\n",
    "Implement the *Thompson Sampling* algorithm (**Algorithm 3.3** in [Bandits]) within the provided bandit algorithm template below. Assume independent priors and that the prior is $\\mathbb{P} = \\text{Beta}(\\alpha_0, \\beta_0)$ with $\\alpha_0 = 1$ and $\\beta_0 = 1$ (i.e. the **Beta-Bernoulli** setting, on **page 35** in [Bandits]).\n",
    "\n",
    "**Note:** There is a typo in the expression for the posterior $\\mathbb{P}_H$ in [Bandits]. It should be $\\text{Beta}(\\alpha_0 + \\text{REW}_H,\\ \\beta_0 + t - \\text{REW}_H)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThompsonSampling(BanditAlgorithmBase):\n",
    "    def __init__(self, T, K):\n",
    "        \"\"\"\n",
    "        Constructor of the bandit algorithm\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        T : int\n",
    "            Horizon\n",
    "        K : int\n",
    "            Number of actions\n",
    "        \"\"\"\n",
    "        \n",
    "        # FILL IN CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def select_action(self):\n",
    "        \"\"\"\n",
    "        Select an action which will be performed in the environment in the \n",
    "        current time step\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        An action index (integer) in [0, K-1]\n",
    "        \"\"\"\n",
    "        \n",
    "        # FILL IN CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def update(self, action, reward):\n",
    "        \"\"\"\n",
    "        Update the bandit algorithm with the reward received from the \n",
    "        environment for the action performed in the current time step\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        action : int\n",
    "            An action index (integer) in [0, K-1]\n",
    "        reward : int\n",
    "            Reward (integer) in {0, 1} (Bernoulli rewards)\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # FILL IN CODE HERE\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the algorithm in the provided environment using the code below (averaging regret over 5 runs). The plot should show sublinear regret with respect to $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "np.random.seed(SEED)\n",
    "ts_df = run_repeated_experiments(ThompsonSampling, SEED)\n",
    "ts_df.plot(x='t', y='regret', title='Thompson Sampling')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 6\n",
    "(6 points)\n",
    "\n",
    "In this theory problem, you will show an intermediary step in the proof for the Bayesian regret bound of *Thompson Sampling* in the [Bandits] book.\n",
    "\n",
    "You are given a $K$-armed bandit problem with rewards in the interval $[0, 1]$. You can assume that $K \\leq T$, where $T$ is the horizon. Additionally, you can assume that **Lemma 1.5** holds (i.e., for this assignment we define $r_t (a) := \\sqrt{\\frac{2  \\log T}{ n_t (a)}}$, and then it holds that $\\text{Pr}\\left\\{ \\mathcal{E} \\right\\} \\geq 1 - \\frac{2}{T^2}$ with $\\mathcal{E} := \\left\\{ \\forall a \\forall t \\;\\; \\vert \\bar{\\mu}_t (a) - \\mu (a) \\vert \\leq r_t (a) \\right\\}$). Then, with $\\text{UCB}_t (a) := \\bar{\\mu}_t (a) + r_t (a)$, show that $\\mathbb{E}\\left[ \\left[ \\text{UCB}_t (a) - \\mu (a) \\right]^{-} \\right] \\leq \\frac{2}{TK}$ (i.e., show that Equation 3.14 in [Bandits], with $\\gamma = 2$, holds for all arms $a$ and rounds $t$).\n",
    "\n",
    "**Note:** $[x]^{-}$ is the negative portion of $x$, i.e., $[x]^{-} = 0$ if $x \\geq 0$ and $[x]^{-} = \\vert x \\vert$ otherwise.\n",
    "\n",
    "**Hint:** Remember that, given a random variable $X$, an event $\\mathcal{E}$ (subset of the sample space) and its complement $\\mathcal{E}^c$, by the tower rule, $\\mathbb{E}\\left[ X \\right] = \\mathbb{E}\\left[ X \\;\\vert\\; \\mathcal{E} \\right] \\cdot \\text{Pr}\\left\\{ \\mathcal{E} \\right\\} + \\mathbb{E}\\left[ X \\;\\vert\\; \\mathcal{E}^c \\right] \\cdot \\text{Pr}\\left\\{ \\mathcal{E}^c \\right\\}$.\n",
    "\n",
    "*Write the solution in the Markdown cell below (use LaTeX-math mode for equations, etc.).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WRITE SOLUTION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "fig.suptitle(\"Final comparison\")\n",
    "ax.set_xlabel(\"t\"); ax.set_ylabel(\"Regret\")\n",
    "dfs = [(ef_df, \"Explore-First\"), (eg_df, \"Epsilon_t-Greedy\"), (ucb1_df, \"UCB1\"), (ts_df, \"Thompson Sampling\")]\n",
    "for df, label in dfs: df.plot(x='t', y='regret',ax = ax, label = label) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: KL-UCB "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(0 points)\n",
    "\n",
    "As we explored in Problem 4, UCB1 relies on Hoeffding's inequality to determine the confidence radius. Hoeffding's inequality assumes that the reward is bounded (or more generally that they are sub-Gaussian). However, in this assignment we know that the rewards are Bernoulli distributed. As such, Hoeffding's inequality is too general to ensure optimal performance. \n",
    "\n",
    "KL-UCB is an upper confidence algorithm that uses the following selection rule (after sampling every arm once):\n",
    "$$\n",
    "a_t = \\argmax_{a} \\max_{\\tilde{\\mu}} \\Big\\{ \\tilde \\mu \\in [0,1] : d(\\bar{\\mu}_t(a), \\tilde \\mu) \\leq \\frac{\\log f(t)}{n_{t}(a)} \\Big\\}\n",
    "$$\n",
    "where $f(t) = 1 + t \\log^2(t)$ and $d(p, q) = p \\log \\frac{p}{q} + (1-p) \\log \\frac{1-p}{1-q}$ is the KL-divergence between two Bernoulli distributions with mean $p \\in (0,1)$ and $q \\in (0,1)$. Note that $d(p,q)$ is undefined for $p$ or $q$ in $\\{0,1\\}$.\n",
    "\n",
    "As a bonus exercise, implement KL-UCB and compare its performance against UCB1. Since KL-UCB relies on tighter concentration inequalities, we would expect KL-UCB to accumulate less regret than UCB1. \n",
    "\n",
    "If you are up for it, try making your implementation computationally efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KLUCB(BanditAlgorithmBase):\n",
    "    def __init__(self, T, K):\n",
    "        \"\"\"\n",
    "        Constructor of the bandit algorithm\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        T : int\n",
    "            Horizon\n",
    "        K : int\n",
    "            Number of actions\n",
    "        \"\"\"\n",
    "        \n",
    "        # FILL IN CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def select_action(self):\n",
    "        \"\"\"\n",
    "        Select an action which will be performed in the environment in the \n",
    "        current time step\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        An action index (integer) in [0, K-1]\n",
    "        \"\"\"\n",
    "        \n",
    "        # FILL IN CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def update(self, action, reward):\n",
    "        \"\"\"\n",
    "        Update the bandit algorithm with the reward received from the \n",
    "        environment for the action performed in the current time step\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        action : int\n",
    "            An action index (integer) in [0, K-1]\n",
    "        reward : int\n",
    "            Reward (integer) in {0, 1} (Bernoulli rewards)\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # FILL IN CODE HERE\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT MODIFY\n",
    "np.random.seed(SEED)\n",
    "klucb_df = run_repeated_experiments(KLUCB, SEED)\n",
    "klucb_df.plot(x='t', y='regret', title='KL-UCB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "fig.suptitle(\"Final comparison\")\n",
    "ax.set_xlabel(\"t\"); ax.set_ylabel(\"Regret\")\n",
    "dfs = [(ef_df, \"Explore-First\"), (eg_df, \"Epsilon_t-Greedy\"), (ucb1_df, \"UCB1\"), (ts_df, \"Thompson Sampling\"), (klucb_df, \"KL-UCB\")]\n",
    "for df, label in dfs: df.plot(x='t', y='regret',ax = ax, label = label) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
