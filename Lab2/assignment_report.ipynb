{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report, assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1)\n",
    "\n",
    "Briefly describe the Q-learning, Double Q-learning, SARSA and Expected SARSA algorithm and how each of them works. Also, briefly describe the dissimilarities and similarities between these agents.\n",
    "\n",
    "\n",
    "Q-Learning is an off-policy algorithm where the optimal action-selection policy is found using a value-based method. It works by iteratively updating Q-values which estimate the expected future rewards for taking a certain action in a given state. The update rule is:\n",
    "\n",
    "\\begin{equation*}\n",
    "Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right]\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\alpha$ is the learning-rate, $\\gamma$ is the discount factor, $r$ is the reward, $\\max_{a'} Q(s', a')$ is the maximum future Q-value for the next state-action pair.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "Double Q-Learning is a variant of Q-Learning that reduces overestimation bias by using two sets of Q-values, $Q_1$ and $Q_2$. The update rules are\n",
    "\n",
    "\\begin{equation*}\n",
    "Q_1(s, a) \\leftarrow Q_1(s, a) + \\alpha \\left[ r + \\gamma Q_2(s', \\arg\\max_{a'} Q_1(s', a')) - Q_1(s, a) \\right]\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "Q_2(s, a) \\leftarrow Q_2(s, a) + \\alpha \\left[ r + \\gamma Q_1(s', \\arg\\max_{a'} Q_2(s', a')) - Q_2(s, a) \\right]\n",
    "\\end{equation*}\n",
    "\n",
    "This separation reduces the overestimation of Q-values and leads to more stable learning.\n",
    "\n",
    "<br>\n",
    "\n",
    "SARSA is an on-policy algorithm, it learns based on the actions actually taken by the agent. The Q-value update rule is\n",
    "\n",
    "\\begin{equation*}\n",
    "Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r + \\gamma Q(s', a') - Q(s, a) \\right]\n",
    "\\end{equation*}\n",
    "\n",
    "where $a'$ is the action taken in the next state $s'$. \n",
    "\n",
    "<br>\n",
    "\n",
    "Expected SARSA is another on-policy, but instead of using the actual action taken in the next state, it uses the expected value of all possible actions according to the current policy. The update rule is\n",
    "\n",
    "\\begin{equation*}\n",
    "Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r + \\gamma \\sum_{a'} \\pi(a'|s') Q(s', a') - Q(s, a) \\right]\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\pi(a'|s')$ is the probability of taking action $a'$ in state $s'$ according to the current policy. Expected SARSA reduces variance compared to SARSA by averaging over possible actions.\n",
    "\n",
    "<br>\n",
    "\n",
    "All four algorithms aim to learn the optimal policy that maximizes the expected cumulative reward for an agent in a given environment. They use value estimates to represent the expected future rewards for state-action pairs and update these estimates based on experience. They all derive their update rules from the Bellman equation, adjusting their value estimates based on the observed rewards and estimated future rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"cumulative_rewards_Q_DIFF.png\" width=650px height=500px />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
