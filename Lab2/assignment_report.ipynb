{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report, assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1)\n",
    "\n",
    "Briefly describe the Q-learning, Double Q-learning, SARSA and Expected SARSA algorithm and how each of them works. Also, briefly describe the dissimilarities and similarities between these agents.\n",
    "\n",
    "\n",
    "Q-Learning is an off-policy algorithm where the optimal action-selection policy is found using a value-based method. It works by iteratively updating Q-values which estimate the expected future rewards for taking a certain action in a given state. The update rule is:\n",
    "\n",
    "\\begin{equation*}\n",
    "Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right]\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\alpha$ is the learning-rate, $\\gamma$ is the discount factor, $r$ is the reward, $\\max_{a'} Q(s', a')$ is the maximum future Q-value for the next state-action pair.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "Double Q-Learning is a variant of Q-Learning that reduces overestimation bias by using two sets of Q-values, $Q_1$ and $Q_2$. The update rules are\n",
    "\n",
    "\\begin{equation*}\n",
    "Q_1(s, a) \\leftarrow Q_1(s, a) + \\alpha \\left[ r + \\gamma Q_2(s', \\arg\\max_{a'} Q_1(s', a')) - Q_1(s, a) \\right]\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "Q_2(s, a) \\leftarrow Q_2(s, a) + \\alpha \\left[ r + \\gamma Q_1(s', \\arg\\max_{a'} Q_2(s', a')) - Q_2(s, a) \\right]\n",
    "\\end{equation*}\n",
    "\n",
    "This separation reduces the overestimation of Q-values and leads to more stable learning.\n",
    "\n",
    "<br>\n",
    "\n",
    "SARSA is an on-policy algorithm, it learns based on the actions actually taken by the agent. The Q-value update rule is\n",
    "\n",
    "\\begin{equation*}\n",
    "Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r + \\gamma Q(s', a') - Q(s, a) \\right]\n",
    "\\end{equation*}\n",
    "\n",
    "where $a'$ is the action taken in the next state $s'$. \n",
    "\n",
    "<br>\n",
    "\n",
    "Expected SARSA is another on-policy, but instead of using the actual action taken in the next state, it uses the expected value of all possible actions according to the current policy. The update rule is\n",
    "\n",
    "\\begin{equation*}\n",
    "Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r + \\gamma \\sum_{a'} \\pi(a'|s') Q(s', a') - Q(s, a) \\right]\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\pi(a'|s')$ is the probability of taking action $a'$ in state $s'$ according to the current policy. Expected SARSA reduces variance compared to SARSA by averaging over possible actions.\n",
    "\n",
    "<br>\n",
    "\n",
    "All four algorithms aim to learn the optimal policy that maximizes the expected cumulative reward for an agent in a given environment. They use value estimates to represent the expected future rewards for state-action pairs and update these estimates based on experience. They all derive their update rules from the Bellman equation, adjusting their value estimates based on the observed rewards and estimated future rewards. Additionally, all the algorithms employ mechanisms to balance exploration and exploitation.\n",
    "\n",
    "Q-learning and Double Q-learning are off-policy methods that rely on the maximum Q-value for future states, while SARSA and Expected SARSA, on the other hand, are on-policy algorithms that update Q-values based on the actual actions taken. Off-policy methods aim to find an optimal policy independent of the current actions, while on-policy methods adapt their learning directly from the agent's chosen actions and improve stability by incorporating the expected value of all actions based on the current policy, rather than just the action taken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"cumulative_rewards_Q_DIFF_MINMAX.png\" width=650px height=500px />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"cumulative_rewards_Q_DIFF.png\" width=650px height=500px />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can observe how different Q_Value initializations effect the performance of the agents. In this partcular case we are using QLearningAgent as our mode to visualize the impact of the different initialization. \n",
    "\n",
    "In this case we are using uniform random distrubution with a min value and a max value for each table, which does bring some inherent randomness to the tests of the agent. We can see from the plot that in this case, a low non-zero Q-value brings the best performance to our model, and going too high or too low is not in our benefit. There is also an argument for why not to use a random distrubution, at least not with an interval that is too large, since a difference of 5 and 10 is very bad in the case of frozen lake for example, where when we get a reward that is 1 only, that may be very confusing for our model dependning on how the q tables are updated with the rewards.\n",
    "\n",
    "In any case we think that a random distrubution of Q-Values is good to encourage exploration and avoiding deadlocks. Especially in models where we only get a reward when we succeed and no negative rewards for anything else.\n",
    "\n",
    "In the case where we used only zeros, as we noticed before during training it could make the same mistakes over and over again, since we do not get any negative rewards in some cases, and for example if the q value is zero, in the case where the q_value is substracted with a negative multiplication of itself, it will never change.\n",
    "\n",
    "In this case we observe the best run of them all with a random distrubution, however, this is part due to luck and we noticed that after several obeservations and runs, using the same value everywhere redndered a more consistant higher cumulative reward.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"cumulative_rewards_subplots_with_colors.png\" width=650px height=500px />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are using uniform Q-values of 0.1, which we observed had the best consistency and high perfromance in the case of a 4x4 frozen lake. We observe that all of the models look very much alike in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4)\n",
    "For the FrozenLake environment, we visualize the greedy policy as plots of the 4x4 grid with the raw Q-values for each state as well as an arrow indicating which optimal direction (action) the states reaches.\n",
    "\n",
    "<img src=\"cool_plot_arrows_pretty_af_ngl.png\" width=650px height=650px />\n",
    "\n",
    "The plot visualizes the q values for each coordinate after a run with the Q-learning agent. The arrows represnt which square will be picked next, the percentage that that arrow will be picked is 95% in this case, with a greedy policy of 95%.\n",
    "\n",
    "We do reach the optimal path for all agents, it is however worth noting in this csae that there are several optimal paths, considering that we use manhattan distance here, and there are several ways to reach the end goal, travelling the same distance. We can for example start by going down or to the right, and we can also see that in our case, looking at the arrows, we reach the optimal path wherever we go. \n",
    "\n",
    "\n",
    "For the RiverSwim environment, we were not sure how to effectively visualize the greedy policy. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5)\n",
    "\n",
    "When testing in this type of environment, we observe that all agenst perform rather alike each other, it would be interesting to do a deeper analysis into more complicacted \"games\" where we could perhaps see a bigger difference between the different agents. We can also conclude that in this case, the initialization of the q-values have a bigger impact on performance than the actual choice of model. Some start-values of Q rendered very poor results, like using a starting value below zero or above one. This is however dependent on the \"game\" or rather how the awards are given. We think that the initialization would probably be different given different rewards, and in cases where we would get negative rewards. For example, if the reward was 2, instead of 1, our performance drop-off from q-values would be after 2 instead of 1. It would be interesting to try more configurations and more games. Using a 8x8 grid, and perhaps see what ghappens if we all of the sudden, the ice braks somewhere and the agents has to adapt to a changed map, how fast could it change and what confuiguration woukd be best for this enviornment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
